{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction:**\n",
        "\n",
        "* statsmodels is a Python library that helps you do statistics and statistical modeling.\n",
        "\n",
        "* It‚Äôs like Excel for serious statistics, but in Python.\n",
        "\n",
        "* Think of it like a tool that helps you answer questions like:\n",
        "\n",
        "  > Does studying more hours IMPROVE test scores?\n",
        "\n",
        "  > How does temperature AFFECT ice cream sales?\n",
        "\n",
        "  > Can we PREDICT next month‚Äôs sales based on past months?\n",
        "\n",
        "* It can do things like:\n",
        "\n",
        "  1. Linear regression\n",
        "\n",
        "  2. Logistic regression\n",
        "\n",
        "  3. Time series analysis\n",
        "\n",
        "  4. ANOVA (comparing groups)\n",
        "\n",
        "  5. Many statistical tests"
      ],
      "metadata": {
        "id": "4hf1cmUWF4HG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing:**"
      ],
      "metadata": {
        "id": "5XDKPyIfGYnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install statsmodels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO6EOQN2GeJL",
        "outputId": "a7c10645-3568-4fa6-e9f9-6b0e5751edbc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.5)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (2.0.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.16.1)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interfaces inside Statsmodels:**\n",
        "\n",
        "Statsmodels is the library. Inside it, there are two ways (interfaces) to use it:\n",
        "\n",
        "1. **Formula API** ‚Üí a convenient way to tell statsmodels what you want, using formulas like Y ~ X1 + X2. It takes column names from DataFrame. So, in it, write a math-like formula, statsmodels handles columns.\n",
        "\n",
        "2. **Data API** (Explicit API) ‚Üí a more manual way, where you give X and Y arrays yourself. It takes arrays/matrices or explicit data. You manually give X and Y arrays, more work, more flexible.\n",
        "\n",
        "So:\n",
        "\n",
        "> Statsmodels = the tool/library\n",
        "\n",
        "> Formula API and Data API = two ways (interfaces) to use that tool\n",
        "\n",
        "Analogy:\n",
        "\n",
        "* Statsmodels is like a car. üöó\n",
        "\n",
        "* Formula API = automatic transmission (easy, beginner-friendly)\n",
        "\n",
        "* Data API = manual transmission (more control, but you have to do more yourself)\n",
        "\n",
        "> formula API and data API me bs itna frk eh k hm dataAPI me khud columns nikaal k datyaframe me se phr dety hain or formula API me hm columns dataframe me e rehny dety hain or model ko naam bta dety hain k yeh yeh cplumn utha lo wo utha leta eh par naam to is me b btany e party hain hmen properly.\n",
        "\n",
        "\n",
        "Automatic = relative term\n",
        "\n",
        "* Formula API ‚Äúautomatically‚Äù picks columns ‚Üí matlab: hum manually X aur Y bana ke alag arrays nahi banate, lekin formula me column ke names phir bhi hume specify karne padte hain.\n",
        "\n",
        "* Agar aapke column names weird hain (jaise X, Y, A1‚Ä¶), aapko formula me wahi names use karne padenge.\n",
        "\n",
        "* Statsmodels name se hi samajhta hai kaun sa dependent (Y) aur kaun sa independent (X) hai.\n",
        "\n"
      ],
      "metadata": {
        "id": "rGlUocRfF4Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1Ô∏è‚É£ Formula API (like R):**"
      ],
      "metadata": {
        "id": "QAqf2yDmF4Ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* You write a formula like in math:\n",
        "\n",
        "          Y ~ X1 + X2\n",
        "\n",
        "1. Y = the thing you want to predict (dependent variable)\n",
        "\n",
        "2. X1, X2 = the things you think affect Y (independent variables)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ei3sfvDDF4OY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Formula Explanation:**\n",
        "\n",
        "This formula 'weight ~ height + age' means:\n",
        "\n",
        "* weight ‚Üí what you want to predict (dependent variable, Y)\n",
        "\n",
        "* ~ ‚Üí ‚Äúis modeled as a function of‚Äù\n",
        "\n",
        "* height + age ‚Üí the things you think affect weight (independent variables, X‚Äôs)\n",
        "\n",
        "<what to predict> ~ <predict using this> + <also using this>\n",
        "\n",
        "In simple words:\n",
        "\n",
        "‚ÄúWeight depends on height and age.‚Äù\n",
        "\n",
        "Statsmodels will read it like:\n",
        "\n",
        "weight = (some number) * height   **+**   (some number) * age **+ (intercept)**\n",
        "\n",
        "\n",
        "> Example:\n",
        "\n",
        "Imagine you want to guess how much a cake will weigh.\n",
        "\n",
        "Ingredients: flour, sugar, eggs\n",
        "\n",
        "Formula: cake_weight ~ flour + sugar + eggs\n",
        "\n",
        "Statsmodels will figure out how much each ingredient affects the weight based on your past cakes.\n",
        "\n"
      ],
      "metadata": {
        "id": "RDdyqrLMK9SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.formula.api as smf\n",
        "import pandas as pd\n",
        "\n",
        "# sample data\n",
        "data = pd.DataFrame({\n",
        "    'height': [150, 160, 170, 180],\n",
        "    'weight': [50, 60, 70, 80],\n",
        "    'age': [20, 25, 30, 35]\n",
        "})\n",
        "\n",
        "# formula API: predict weight from height and age\n",
        "model = smf.ols('weight ~ height + age', data=data).fit()\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWoLhaswK6sY",
        "outputId": "01c4d8e6-c68a-4feb-81be-aec117c1dcbe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 weight   R-squared:                       1.000\n",
            "Model:                            OLS   Adj. R-squared:                  1.000\n",
            "Method:                 Least Squares   F-statistic:                 2.713e+29\n",
            "Date:                Tue, 02 Sep 2025   Prob (F-statistic):           3.69e-30\n",
            "Time:                        11:51:25   Log-Likelihood:                 118.83\n",
            "No. Observations:                   4   AIC:                            -233.7\n",
            "Df Residuals:                       2   BIC:                            -234.9\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept     -0.0322   1.14e-16  -2.83e+14      0.000      -0.032      -0.032\n",
            "height         0.0912   9.78e-16   9.32e+13      0.000       0.091       0.091\n",
            "age            1.8176   5.77e-15   3.15e+14      0.000       1.818       1.818\n",
            "==============================================================================\n",
            "Omnibus:                          nan   Durbin-Watson:                   0.027\n",
            "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.963\n",
            "Skew:                          -1.155   Prob(JB):                        0.618\n",
            "Kurtosis:                       2.333   Cond. No.                     1.74e+19\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 3.72e-34. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 4 samples were given.\n",
            "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Understanding Results:**\n",
        "\n",
        "* Weight is dependent/target variable. We are using model Ordinary Least Squares (OLS).\n",
        "* No. Observations means number of instances (rows) used which is 4. We have 3 Coefficients that are (Intercept + height + age).\n",
        "* Df (Degree of Freedom)\n",
        "* Residuals = ‚Äúerrors‚Äù ya ‚Äúdifferences between actual and predicted values‚Äù\n",
        "\n",
        "## **1. Df Residuals:**\n",
        "\n",
        "basically how many pieces of information are left after fitting the model. mtlb hm kitny elements use krye hain or baki kitny bach rahy. hm 3 use krye hain to predict 1. so Df residuals  = 4-3 = 1 means 1 bach raha eh baki. 3 hmne use krliye hmary data me se. **bold text**\n",
        "Sirf ek chhoti si clarification: Intercept bhi ek coefficient count hota hai, isliye humne 3 include kiye.\n",
        "\n",
        "      Df Residuals = Number of observations ‚Äì Number of coefficients\n",
        "\n",
        "But, in our result it says 2 here because of tiny dataset & perfect correlation (height & age are almost linearly dependent). Mtlb dono variable height and age dono barh rye hain. same effect day rye hain. Linear regression me, agar 2 variables same info de rahe hain, model nahi bata sakta clearly kaun sa zyada effect kar raha hai. agr aik barh rae hoti aik decrease ya mix trha se to phr keh skty thy k yh dono difffernt hain phr result 4-3 = 1 ata. mgr ab wo dono ko same treat kr raha eh so 4 - 2 (here 1 intercept and 1 heightAndAge).\n",
        "\n",
        "Basically: there‚Äôs barely any ‚Äúfreedom left‚Äù to check errors ‚Üí warns you the dataset is too small.\n",
        "\n"
      ],
      "metadata": {
        "id": "K0eDKly0MhOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Df Model (Degrees of Freedom of Model)**\n",
        "\n",
        "Df Model = How many predictors the model is actually using to explain Y\n",
        "\n",
        "Normally, we count number of predictors (height + age) = 2\n",
        "\n",
        "Here statsmodels shows 1 because:\n",
        "\n",
        "Height & age are almost perfectly correlated in this tiny dataset\n",
        "\n",
        "Model can‚Äôt separate their effects ‚Üí effectively only 1 independent direction explains weight\n",
        "\n",
        "That‚Äôs why you see 1 instead of 2 ‚Üí warning about multicollinearity.\n",
        "\n",
        "### **Multicollinearity:**\n",
        "\n",
        "* Jab do ya do se zyada independent variables (X‚Äôs) ek dusre ke saath bohot closely related ho jate hain mtlb Multicollinearity = predictors ek dusre se itne related ho gaye ke model ko pata nahi kaun zyada effect kar raha hai\n",
        "\n",
        "* Height aur age dono same pattern follow kar rahe hain\n",
        "\n",
        "* Agar model ko ‚Äúweight ko predict karna hai‚Äù ‚Üí model confuse ho jata hai:\n",
        "\n",
        "> ‚ÄúWeight height ki wajah se badh raha hai ya age ki wajah se?‚Äù\n",
        "\n",
        "**Effect of multicollinearity:**\n",
        "\n",
        "* Coefficients become unstable ‚Üí numbers (b1, b2) ka meaning unclear\n",
        "\n",
        "* Standard errors become huge ‚Üí p-values unreliable\n",
        "\n",
        "* Model can still predict Y, but it can‚Äôt tell you clearly which X is more important\n"
      ],
      "metadata": {
        "id": "iyY3EIUmSbwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Covariance Type: Nonrobust**\n",
        "\n",
        "Jab regression me cov_type = 'nonrobust' likha hota hai, iska matlab hai:\n",
        "\n",
        "* ‚ÄúHum normal calculation use kar rahe hain aur maan rahe hain ke data me koi weird cheez (jaise unequal errors) nahi hai.‚Äù\n",
        "\n",
        "* Matlab, standard errors aur p-values jo mile hain, wo sirf tab sahi hain agar data normal ho aur errors barabar hon.\n",
        "\n",
        "* Agar data me gadbad hai (errors barabar nahi, ya weird distribution), to robust use karte hain.\n",
        "\n",
        "* By default statsmodels uses nonrobust. But if you want robust standard errors (to handle heteroscedasticity, outliers, etc.), you can set it manually:\n",
        "\n",
        "      model = smf.ols('weight ~ height + age', data=data).fit(cov_type='HC3')\n",
        "\n",
        "cov_type='HC3' ‚Üí robust standard errors (more reliable if residuals are not perfect)\n",
        "\n",
        "> In simple analogy\n",
        "\n",
        "> Default = ‚ÄúI assume your errors are nice & clean‚Äù ‚Üí nonrobust\n",
        "\n",
        "> Robust = ‚ÄúI don‚Äôt trust errors completely, let‚Äôs adjust standard errors‚Äù\n",
        "\n"
      ],
      "metadata": {
        "id": "D_h0yENAbDCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. R-Square and Adjusted R-Square:**\n",
        "\n"
      ],
      "metadata": {
        "id": "4xQ5_Hc3bDEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* R¬≤ = ‚Äúmodel ne kitna accurately target variable predict kiya‚Äù. mtlb kitna variation model explain kar paa raha hai.\n",
        "\n",
        "* Weight perfectly height aur age se linear follow karta hai\n",
        "\n",
        "* Model ne har ek point bilkul sahi predict kar diya ‚Üí R¬≤ = 1.0\n",
        "\n",
        "* Matlab: 100% variation explained ‚Üí model ne weight ke har change ko explain kar diya\n",
        "\n",
        "* Problem: Agar tum random useless variables add karo, R¬≤ hamesha thoda badh jata hai, chahe wo variable kaam ka na ho.\n",
        "\n",
        "* Example:\n",
        "\n",
        "> Tumhare paas weight predict karna hai ‚Üí height aur age use karo ‚Üí R¬≤ = 0.9\n",
        "Ab tum extra variable add kar do, jaise shoe size ‚Üí R¬≤ automatically 0.91 ho gaya\n",
        "Matlab R¬≤ badh gaya, lekin ‚Äúshoe size‚Äù ka weight pe real effect nahi hai ‚Üí ye misleading hai.\n",
        "\n",
        "\n",
        "**Adjusted R¬≤ solution:**\n",
        "\n",
        "* Adjusted R¬≤ = R¬≤ ko adjust karta hai number of variables aur observations ke hisaab se\n",
        "\n",
        "* Matlab: ‚ÄúSirf useful variables ka effect count karo, useless variables ignore karo‚Äù\n",
        "\n",
        "Formula (simplified version):\n",
        "      \n",
        "      Adjusted R¬≤ = 1 ‚Äì (1 ‚Äì R¬≤) * (n ‚Äì 1)/(n ‚Äì p ‚Äì 1)\n",
        "n = observations\n",
        "\n",
        "p = number of predictors\n",
        "\n",
        "* **Super simple analogy**\n",
        "\n",
        "* Tumhare paas lego blocks aur unka weight = height + age se determine hota hai\n",
        "\n",
        "* R¬≤ check karta hai ‚Üí ‚ÄúKitna weight model ne explain kiya‚Äù\n",
        "\n",
        "* Agar tum extra useless block add karo ‚Üí R¬≤ thoda badh jaata hai\n",
        "\n",
        "* Adjusted R¬≤ ‚Üí ‚ÄúUseless block ka effect ignore karo, sirf useful blocks count karo‚Äù\n",
        "\n",
        "**One line version**\n",
        "\n",
        "* Adjusted R¬≤ = R¬≤ ka smarter version, jo sirf meaningful variables ka effect dikhata hai, useless variables ignore karta hai\n",
        "\n"
      ],
      "metadata": {
        "id": "129f9GwobDG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. F-statistics and Prob(F-statistics):**\n",
        "\n",
        "* F-statistic ‚Üí check karta hai ‚Üí model kitna achha fit hai compare to baseline model (jo bas average predict karta hai) -> ‚ÄúTumhara model random guess se better hai ya nahi?‚Äù\n",
        "\n",
        "* Bigger F-statistic = better model\n",
        "\n",
        "* Prob (F-statistic) = probability that your model is NOT better than random guessing\n",
        "\n",
        "* Small number (like 0.0001) ‚Üí matlab ‚Äúbohot almost impossible ki model random guess jaisa hai‚Äù ‚Üí model really works\n",
        "\n",
        "Prob F = bohot choti ‚Üí matlab model actual me kaam kar raha hai, bas chance se guessing nahi kr raha\n",
        "\n"
      ],
      "metadata": {
        "id": "LdkDYWJPuJKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Log Likelihood, AIC, BIC**\n",
        "\n",
        "* ‚Äúhow well did your guesses match the real weights?‚Äù\n",
        "> Better guesses ‚Üí bigger Log-Likelihood\n",
        "\n",
        "> Bad guesses ‚Üí smaller Log-Likelihood\n",
        "\n",
        "**AIC (Akaike Information Criterion):**\n",
        "\n",
        "* What it is: A number that balances model fit and model complexity\n",
        "\n",
        "      AIC = 2*(number of parameters) - 2*log-likelihood\n",
        "\n",
        "* Smaller AIC ‚Üí better model\n",
        "\n",
        "**Analogy:**\n",
        "\n",
        "* You can have a simple model (2 ingredients) or a complex model (10 ingredients).\n",
        "\n",
        "* Complex model might fit perfectly, but too many ingredients = bad idea\n",
        "\n",
        "* AIC = ‚Äúscore that punishes too many ingredients‚Äù\n",
        "\n",
        "* Goal: smallest score ‚Üí best balance between fit & simplicity\n",
        "\n",
        "**BIC (Bayesian Information Criterion)**\n",
        "\n",
        "* Similar to AIC, but punishes complexity more strictly, especially if dataset is small\n",
        "\n",
        "Formula (conceptually):\n",
        "\n",
        "      BIC = log(n)*(number of parameters) - 2*log-likelihood\n",
        "\n",
        "Analogy:\n",
        "\n",
        "Like AIC, but your teacher is stricter about adding too many ingredients\n",
        "\n",
        "So BIC encourages even simpler models\n",
        "\n",
        "| Term           | What it means                       | Good/bad?        |\n",
        "| -------------- | ----------------------------------- | ---------------- |\n",
        "| Log-Likelihood | How likely model predicted the data | Bigger = better  |\n",
        "| AIC            | Balance fit vs complexity           | Smaller = better |\n",
        "| BIC            | Stricter balance fit vs complexity  | Smaller = better |\n",
        "\n",
        "‚úÖ Quick tip:\n",
        "\n",
        "Log-Likelihood ‚Üí tells how well model fits\n",
        "\n",
        "AIC/BIC ‚Üí tells fit + simplicity, helps choose best model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LRaQRDi3uJMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. coef, std err, t, P>|t|,[0.025 0.975]**\n",
        "\n",
        "* **Coeff** is the number that tells effect of each independent variable on target variable. e.g. har 1 unit height badhne par weight 0.0912 unit badhta hai, har 1 unit age badhne par weight 1.8176 unit badhta hai, har 1 unit intercept badhne par weight -0.0322 unit kam hai.\n",
        "\n",
        "weight = -0.032 + 0.091*height + 1.818*age\n",
        "\n",
        "* **std err:** Ye uncertainty ka measure hai har coefficient ka. Tiny numbers (1e-16 etc) ‚Üí matlab data perfect fit kar raha hai, almost no error\n",
        "\n",
        "* **T-test** check karta hai:\n",
        "üëâ \"Kya yeh coefficient actually zero se different hai?\"\n",
        "  > Agar coefficient = 0 hota, matlab ‚Äúheight/age ka koi effect hi nahi weight pe.‚Äù\n",
        "\n",
        "  > Agar coefficient ‚â† 0 hai, matlab ‚Äúhaan, iska kuch effect hai.‚Äù\n",
        "\n",
        "        Formula: t = coefficient √∑ standard error\n",
        "\n",
        "  > Big t-value ‚Üí matlab effect kaafi strong hai (zero se door hai).\n",
        "\n",
        "  > Small t-value ‚Üí shayad effect zero hi ho.\n",
        "\n",
        "* **3. P>|t| (p-value)::**\n",
        "\n",
        "Ye ek probability hai.\n",
        "\n",
        "üëâ ‚ÄúKitna chance hai ki coefficient bas random noise hai aur asal me zero hi hai?‚Äù\n",
        "\n",
        "Chhota p-value (jaise < 0.05) ‚Üí matlab chance bahut kam hai ki effect zero hai ‚Üí coefficient real effect hai.\n",
        "\n",
        "Bada p-value (jaise 0.5) ‚Üí matlab chance zyada hai ki effect bas random hai ‚Üí coefficient useless.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Socho tum science fair me experiment karte ho:\n",
        "\n",
        "Tumne dekha ki ‚Äúdoodh peene se height badhti hai.‚Äù\n",
        "\n",
        "Coefficient = +2 cm (ye effect).\n",
        "\n",
        "T-test = check karna ki ‚Äúye sach me doodh ki wajah se hai ya bas coincidence?‚Äù\n",
        "\n",
        "P-value = chance of coincidence.\n",
        "\n",
        "Agar p-value chhoti hai ‚Üí matlab doodh sach me kaam kar raha hai.\n",
        "Agar p-value badi hai ‚Üí matlab doodh ka koi asar nahi, bas random bachhe waise hi lambe hue.\n",
        "\n"
      ],
      "metadata": {
        "id": "r534xEURuJOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Omnibus, Prob(Omnibus), Skew, Kurtosis, Durbin Watson, Jarque-Bera (JB), Prob(JB), Cond. No.**\n",
        "\n",
        "Ab hum regression ke result ke bottom section me aa gaye hain.\n",
        "Ye mostly diagnostic tests hote hain ‚Äì model ka health checkup jese.\n",
        "\n",
        "## **A. Omnibus & Prob(Omnibus)**\n",
        "\n",
        "* Ye ek test hai: \"Model ke residuals (errors) normal distribution jese lag rahe hain ya nahi?\"\n",
        "\n",
        "* ‚ÄúResiduals‚Äù = model ke predict kiye hue values aur asli values ka difference.\n",
        "\n",
        "* Agar residuals normal ho ‚Üí good model assumption.\n",
        "\n",
        "* NaN aa raha hai kyunki tumhara dataset bahut chhota hai (4 rows). Itna chhota sample pe test kaam nahi karta.\n",
        "\n",
        "* Agar bada dataset hota, aur Prob(Omnibus) < 0.05 hota ‚Üí matlab residuals normal nahi hain (buri baat).\n",
        "\n",
        "\n",
        "## **B. Jarque-Bera (JB) & Prob(JB)**\n",
        "\n",
        "* Ye bhi ek test hai residuals normal hain ya nahi check karne ka. Ye bhi residuals ki normality check karta hai (Omnibus ka bhai üòÖ).\n",
        "\n",
        "* JB test dekhta hai skewness aur kurtosis ke basis pe.\n",
        "\n",
        "* Prob(JB) = chance ke residuals normal hain.\n",
        "\n",
        "* Yahan Prob(JB) = 0.618 (bahut bada hai, 0.05 se upar) ‚Üí matlab residuals thik thak normal hain (good).\n",
        "\n",
        "## **C. Skew**\n",
        "\n",
        "* Residuals kitne tilt hue hain left ya right.\n",
        "\n",
        "* Skew = 0 ‚Üí perfectly balanced.\n",
        "\n",
        "* Negative skew (jaise -1.155) ‚Üí residuals thoda left me tilt hain.\n",
        "\n",
        "* Thoda sa skew chalta hai, bahut zyada ho to problem hoti hai.\n",
        "\n",
        "## **D. Kurtosis**\n",
        "\n",
        "* Ye batata hai distribution ki ‚Äúpeakedness‚Äù ‚Üí matlab curve kitna sharp ya flat hai compared to normal distribution.\n",
        "\n",
        "* Normal distribution me kurtosis ~3 hoti hai.\n",
        "\n",
        "* Tumhare result me 2.333 ‚Üí thoda flat (lekin near 3, so fine).\n",
        "\n",
        "\n",
        "## **E. Durbin-Watson**\n",
        "\n",
        "* Ye test karta hai: ‚ÄúResiduals ek dusre se related (correlated) hain kya?‚Äù\n",
        "\n",
        "* Range = 0 to 4.\n",
        "\n",
        " > ~2 ‚Üí perfect (no autocorrelation).\n",
        "\n",
        " > <1 ‚Üí strong positive autocorrelation (residuals line wise linked hain).\n",
        "\n",
        " > 3 ‚Üí negative autocorrelation.\n",
        "\n",
        "Tumhare result me = 0.027 ‚Üí matlab bahut strong autocorrelation hai üòÖ (galat sign ki nishani, mostly kyunki data tiny aur perfectly correlated hai).\n",
        "\n",
        "## **F. Cond. No. (Condition Number)**\n",
        "\n",
        "* Ye multicollinearity (variables ek dusre se almost copy) detect karta hai.\n",
        "\n",
        "* Agar cond. no. bahut bada (> 30,000) ho ‚Üí multicollinearity ka doubt.\n",
        "\n",
        "* Tumhare result me = 1.74e+19 (bahut hii huge) ‚Üí matlab tumhare predictors (height & age) almost same information de rahe hain ‚Üí isi liye collinearity warning aayi thi.\n",
        "\n",
        "* Ye check karta hai multicollinearity (predictors zyada similar to nahi).\n",
        "\n",
        "* Agar Cond. No. bahut bada ho (jaise 1.74e+19) ‚Üí matlab predictors ek dusre ke saath lagbhag linearly dependent hain.\n",
        "\n",
        "\n",
        "##‚úÖ Super Simple Recap:\n",
        "\n",
        "Omnibus & JB = normality tests.\n",
        "\n",
        "Skew & Kurtosis = residuals ka shape.\n",
        "\n",
        "Durbin-Watson = autocorrelation check.\n",
        "\n",
        "Cond. No. = multicollinearity check.\n",
        "\n"
      ],
      "metadata": {
        "id": "CGkj9UCPuJQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Notes:**\n",
        "\n",
        "üëâ Ek line me summary:\n",
        "Tumhara dataset bohat chhota hai aur predictors strongly related hain, is wajah se model ki reliability doubtful hai."
      ],
      "metadata": {
        "id": "imGLIgx8T5T1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--------------------------- Appendices:---------------------------**\n",
        "\n",
        "## **1. P-VALUE:**\n",
        "\n",
        "A p-value is a number that helps you understand whether your results are statistically significant or result you observed might just be a coincidence.\n",
        "\n",
        "**What it measures:**\n",
        "\n",
        "* It measures the probability of observing your data, or something more extreme, if the null hypothesis is true.\n",
        "\n",
        "* The null hypothesis usually says: ‚ÄúThere is no effect‚Äù or ‚ÄúThere is no difference.‚Äù This is the ‚Äúdefault‚Äù idea that nothing special is happening.\n",
        "\n",
        "* ‚ÄúIf the null hypothesis were true (nothing special is happening), how likely is it to see the results I got‚Äîor results even more unusual than this?‚Äù\n",
        "\n",
        "**How to interpret it:**\n",
        "\n",
        "* Small p-value (usually ‚â§ 0.05): Strong evidence against the null hypothesis ‚Üí you might reject the null. MTLB EFFECT TO HAI.\n",
        "\n",
        "* Large p-value (usually > 0.05): Weak evidence against the null ‚Üí you fail to reject the null. mtlb wakai koi khaas effect ni.\n",
        "\n",
        "**Important:**\n",
        "\n",
        "* A small p-value does not prove your own hypothesis is true; it just suggests your data is not likely under the null hypothesis.\n",
        "\n",
        "* A large p-value does not prove the null hypothesis is true; it just means your data isn‚Äôt strong enough to reject it.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Suppose you test if a new drug lowers blood pressure.\n",
        "\n",
        "Null hypothesis: ‚ÄúThe drug has no effect.‚Äù\n",
        "\n",
        "You get p = 0.03 ‚Üí there‚Äôs only a 3% chance of seeing this result if the drug truly had no effect, so you might conclude the drug is effective.\n"
      ],
      "metadata": {
        "id": "ah1KrLVwT5oH"
      }
    }
  ]
}